{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hESenvAaWSl9",
        "outputId": "f75c5517-1fea-4363-a7a0-740210e53336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vmas in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from vmas) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from vmas) (2.6.0+cu124)\n",
            "Requirement already satisfied: pyglet<=1.5.27 in /usr/local/lib/python3.11/dist-packages (from vmas) (1.5.27)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from vmas) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from vmas) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->vmas) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->vmas) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->vmas) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install vmas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.categorical import Categorical\n",
        "import vmas\n",
        "import gym\n",
        "import itertools\n",
        "\n",
        "class GraphSageLayer(nn.Module):\n",
        "    def __init__(self, dim_in: int,\n",
        "                 dim_out: int,\n",
        "                 agg_type: str):\n",
        "        super().__init__()\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.agg_type = agg_type\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "        if agg_type == 'gcn':\n",
        "            self.weight = nn.Linear(dim_in, dim_out, bias=False)\n",
        "            self.bias   = nn.Linear(dim_in, dim_out, bias=False)\n",
        "        elif agg_type == 'mean':\n",
        "            self.weight = nn.Linear(2 * dim_in, dim_out, bias=False)\n",
        "        elif agg_type == 'maxpool':\n",
        "            self.linear_pool = nn.Linear(dim_in, dim_in, bias=True)\n",
        "            self.weight      = nn.Linear(2 * dim_in, dim_out, bias=False)\n",
        "        else:\n",
        "            raise RuntimeError(f\"Unknown aggregation type: {agg_type}\")\n",
        "\n",
        "    def forward(self, feat: torch.Tensor,\n",
        "                edge: torch.Tensor,\n",
        "                degree: torch.Tensor) -> torch.Tensor:\n",
        "        # feat: [N, dim_in], edge: [E,2], degree: [N]\n",
        "\n",
        "        agg_vector = torch.zeros_like(feat)\n",
        "\n",
        "        if self.agg_type == 'gcn':\n",
        "            agg_vector.index_add_(0, edge[:,1], feat[edge[:,0]])\n",
        "            inv = (1.0 / degree.clamp(min=1)).unsqueeze(-1)\n",
        "            out = self.act(self.weight(agg_vector * inv) + self.bias(feat))\n",
        "\n",
        "        elif self.agg_type == 'mean':\n",
        "            agg_vector.index_add_(0, edge[:,1], feat[edge[:,0]])\n",
        "            inv = (1.0 / degree.clamp(min=1)).unsqueeze(-1)\n",
        "            cat = torch.cat([agg_vector * inv, feat], dim=-1)\n",
        "            out = self.act(self.weight(cat))\n",
        "\n",
        "        else:  # 'maxpool'\n",
        "            src = self.act(self.linear_pool(feat))[edge[:,0]]\n",
        "            idx = edge[:,1].unsqueeze(-1).expand_as(src)\n",
        "            agg_vector.scatter_reduce_(0, idx, src, reduce='amax', include_self=False)\n",
        "            cat = torch.cat([agg_vector, feat], dim=-1)\n",
        "            out = self.act(self.weight(cat))\n",
        "\n",
        "        return F.normalize(out, p=2, dim=-1)\n",
        "\n",
        "def build_edge_lists(coords, n_agents, threshold: float):\n",
        "    edges = [(i, i) for i in range(n_agents)]\n",
        "    for i, j in itertools.combinations(range(n_agents), 2):\n",
        "        dist = torch.dist(coords[i], coords[j], p=2)\n",
        "        if dist <= threshold:\n",
        "            edges.append((i, j))\n",
        "    return edges\n",
        "\n",
        "def build_knn_edge_lists(coords, n_agents, k: int):\n",
        "    edges = []\n",
        "    dist = torch.cdist(coords, coords, p=2)\n",
        "    dist.fill_diagonal_(float('inf'))\n",
        "    knn_idx = torch.topk(dist, k, largest=False).indices\n",
        "    edges = [(i, i) for i in range(n_agents)]\n",
        "    for i in range(n_agents):\n",
        "        for j in knn_idx[i].tolist():\n",
        "            edges.append((i, j))\n",
        "    return edges"
      ],
      "metadata": {
        "id": "0nD6u8giXb-o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import deque\n",
        "import vmas\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from distutils.util import strtobool\n",
        "\n",
        "def parse_args():\n",
        "    class Args:\n",
        "        # Experiment setup\n",
        "        exp_name = \"notebook_run\"\n",
        "        scenario = \"navigation\"\n",
        "        learning_rate = 1e-4\n",
        "        seed = 1\n",
        "        total_timesteps = 50_000_000\n",
        "        torch_deterministic = True\n",
        "        cuda = True\n",
        "        track = False\n",
        "        wandb_project_name = \"graph-ml-projects\"\n",
        "        wandb_entity = None\n",
        "\n",
        "        # GNN specific arguments\n",
        "        agg_type = \"gcn\"\n",
        "        hidden_dim = 64\n",
        "        dist = 0.1\n",
        "\n",
        "        # Algorithm hyperparameters\n",
        "        num_agents = 4\n",
        "        num_envs = 600\n",
        "        num_steps = 100\n",
        "        anneal_lr = True\n",
        "        gae = True\n",
        "        gamma = 0.99\n",
        "        gae_lambda = 0.95\n",
        "        num_minibatches = 45\n",
        "        update_epochs = 4\n",
        "        norm_adv = True\n",
        "        clip_coef = 0.2\n",
        "        clip_vloss = True\n",
        "        ent_coef = 0.01\n",
        "        vf_coef = 0.5\n",
        "        max_grad_norm = 0.5\n",
        "        target_kl = None\n",
        "\n",
        "        # Derived values\n",
        "        batch_size = num_envs * num_steps  # 600 * 100 = 60000\n",
        "        minibatch_size = batch_size // num_minibatches  # 60000 // 45 â‰ˆ 1333\n",
        "\n",
        "    return Args()\n",
        "\n",
        "def make_env(scenario, num_envs, continuous_actions, seed, device, n_agents):\n",
        "    return vmas.make_env(\n",
        "        scenario=scenario,\n",
        "        num_envs=num_envs,\n",
        "        device=device,\n",
        "        continuous_actions=continuous_actions,\n",
        "        seed=seed,\n",
        "        n_agents=n_agents,\n",
        "        max_steps=100,\n",
        "    )\n",
        "\n",
        "def add_agent_id(obs, num_envs, num_agents, device):\n",
        "    agent_ids = torch.eye(num_agents, device=device)  # [num_agents, num_agents]\n",
        "    agent_ids = agent_ids.unsqueeze(0).repeat(num_envs, 1, 1)  # [num_envs, num_agents, num_agents]\n",
        "    agent_ids = agent_ids.view(-1, num_agents)  # [num_envs * num_agents, num_agents]\n",
        "    return torch.cat([obs, agent_ids], dim=-1)  # [num_envs * num_agents, obs_dim + num_agents]\n",
        "\n",
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "    torch.nn.init.orthogonal_(layer.weight, std)\n",
        "    torch.nn.init.constant_(layer.bias, bias_const)\n",
        "    return layer\n",
        "\n",
        "class GraphSageAgent(nn.Module):\n",
        "    def __init__(self, obs_dim, act_dim, hidden_dim, dist=1.0, agg_type='gcn'):\n",
        "        super().__init__()\n",
        "        self.dist = dist\n",
        "        # GraphSAGE policy\n",
        "        self.gsage1 = GraphSageLayer(obs_dim, hidden_dim, agg_type=agg_type)\n",
        "        self.gsage2 = GraphSageLayer(hidden_dim, hidden_dim, agg_type=agg_type)\n",
        "        self.policy_head = layer_init(nn.Linear(hidden_dim, act_dim), std=0.01)\n",
        "        # Critic: MLP\n",
        "        self.critic = nn.Sequential(\n",
        "            layer_init(nn.Linear(hidden_dim, 256)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(256, 256)), nn.Tanh(),\n",
        "            layer_init(nn.Linear(256, 1), std=1.0)\n",
        "        )\n",
        "\n",
        "    def get_value(self, communicated_obs: torch.Tensor) -> torch.Tensor:\n",
        "        # communicated_obs: [-1, obs_dim]\n",
        "        return self.critic(communicated_obs)\n",
        "\n",
        "    def get_communicated_obs_slow(self, x: torch.Tensor, positions):\n",
        "        # x: [-1, num_agents, obs_dim]\n",
        "        # positions: [-1, num_agents, 2]\n",
        "        device = x.device\n",
        "        num_agents = x.shape[1]\n",
        "        # apply GCN per env\n",
        "        h_list = []\n",
        "        for e in range(x.shape[0]):\n",
        "            feat_e = x[e] # [num_agents, obs_dim]\n",
        "            coord_e = positions[e] # [num_agents, 2]\n",
        "            edges_e = build_edge_lists(coord_e, num_agents, self.dist)\n",
        "            edges_e = torch.tensor(edges_e).to(device) # [num_edges, 2]\n",
        "            degree_e = torch.bincount(edges_e[:,1], minlength=num_agents).to(device) # [num_agents, ]\n",
        "            h_e = self.gsage1(feat_e, edges_e, degree_e)\n",
        "            h_e = self.gsage2(h_e, edges_e, degree_e) # [num_agents, obs_dim]\n",
        "            h_list.append(h_e)\n",
        "        # flatten back\n",
        "        h = torch.cat(h_list, dim=0)  # [-1, 64]\n",
        "        return h\n",
        "\n",
        "    def get_communicated_obs(self, x: torch.Tensor, positions: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x: [num_envs, num_agents, obs_dim]\n",
        "        positions: [num_envs, num_agents, pos_dim]  (e.g. pos_dim=2)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        num_envs, n_agents, feat_dim = x.shape\n",
        "\n",
        "        # 1) Flatten node features: [N, feat_dim], N = num_envs * n_agents\n",
        "        x_flat = x.view(-1, feat_dim)  # N x feat_dim\n",
        "        dist_mat = torch.cdist(positions, positions, p=2)\n",
        "        mask = dist_mat <= self.dist  # [E, N, N]\n",
        "        env_idx, src_idx, dst_idx = mask.nonzero(as_tuple=True)\n",
        "        src_flat = env_idx * n_agents + src_idx\n",
        "        dst_flat = env_idx * n_agents + dst_idx\n",
        "        edges = torch.stack([src_flat, dst_flat], dim=1).to(device)  # E_total x 2\n",
        "        N = num_envs * n_agents\n",
        "        degree = torch.bincount(dst_flat, minlength=N).to(device)\n",
        "\n",
        "        h = self.gsage1(x_flat, edges, degree)\n",
        "        h = self.gsage2(h, edges, degree)  # still N x hidden_dim\n",
        "\n",
        "        return h  # [num_envs * num_agents, hidden_dim]\n",
        "\n",
        "    def get_action_and_value(self, communicated_obs, action=None):\n",
        "        # communicated_obs: [-1, obs_dim]\n",
        "        logits = self.policy_head(communicated_obs)\n",
        "        dist = Categorical(logits=logits)\n",
        "        if action is None:\n",
        "            action = dist.sample()\n",
        "        return action, dist.log_prob(action), dist.entropy(), self.critic(communicated_obs)\n",
        "\n",
        "    def get_action(self, x, deterministic=True):\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    run_name = f\"{args.scenario}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "    if args.track:\n",
        "        import wandb\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    envs = make_env(\n",
        "        scenario=args.scenario,\n",
        "        num_envs=args.num_envs,\n",
        "        continuous_actions=False, # only consider discrete action space\n",
        "        seed=args.seed,\n",
        "        device=device,\n",
        "        n_agents=args.num_agents,\n",
        "    )\n",
        "\n",
        "    # check dim of env\n",
        "    obs_list = envs.reset()\n",
        "    obs_dim = obs_list[0].shape[-1]\n",
        "    act_dim = envs.action_space[0].n\n",
        "\n",
        "    agent = GraphSageAgent(obs_dim=obs_dim + args.num_agents, act_dim=act_dim, hidden_dim=args.hidden_dim, dist=args.dist, agg_type=args.agg_type).to(device)\n",
        "    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "    # Storage setup\n",
        "    obs = torch.zeros((args.num_steps, args.num_envs * args.num_agents, args.hidden_dim)).to(device)\n",
        "    actions = torch.zeros((args.num_steps, args.num_envs * args.num_agents)).to(device)\n",
        "    logprobs = torch.zeros((args.num_steps, args.num_envs * args.num_agents)).to(device)\n",
        "    rewards = torch.zeros((args.num_steps, args.num_envs * args.num_agents)).to(device)\n",
        "    dones = torch.zeros((args.num_steps, args.num_envs * args.num_agents)).to(device)\n",
        "    values = torch.zeros((args.num_steps, args.num_envs * args.num_agents)).to(device)\n",
        "\n",
        "    # start the game\n",
        "    global_step = 0\n",
        "    SAVE_INTERVAL = 10_000_000\n",
        "    episode_returns = np.zeros(args.num_envs, dtype=np.float32)\n",
        "    episode_lengths = np.zeros(args.num_envs, dtype=np.int32)\n",
        "    return_queue = deque(maxlen=100)\n",
        "    length_queue = deque(maxlen=100)\n",
        "    start_time = time.time()\n",
        "    next_obs_list = envs.reset(seed=args.seed) # (num_envs, obs_dim) per agents\n",
        "    next_obs = torch.stack(next_obs_list, dim=1).to(device) # [num_envs, num_agents, obs_dim]\n",
        "    next_obs = next_obs.view(-1, obs_dim) # [num_envs * num_agents, obs_dim]\n",
        "    next_obs = add_agent_id(next_obs, args.num_envs, args.num_agents, device) # [num_envs * num_agents, obs_dim + num_agents]\n",
        "    next_gnn_obs = next_obs.view(-1, args.num_agents, obs_dim + args.num_agents) # [-1, num_agents, obs_dim]\n",
        "\n",
        "    next_done = torch.zeros(args.num_envs * args.num_agents).to(device)\n",
        "    num_updates = args.total_timesteps // args.batch_size\n",
        "\n",
        "    positions = []\n",
        "    for e in range(args.num_envs):\n",
        "        coords = torch.stack([agent.state.pos[e] for agent in envs.agents], dim=0).to(device)  # [n_agents, pos_dim]\n",
        "        positions.append(coords)\n",
        "    positions = torch.stack(positions, dim=0).to(device)  # [-1, num_agents, 2]\n",
        "\n",
        "    for update in range(1, num_updates + 1):\n",
        "        # Annealing the rate if instructed to do so.\n",
        "        if args.anneal_lr:\n",
        "            frac = 1.0 - (update - 1.0) / num_updates\n",
        "            lrnow = frac * args.learning_rate\n",
        "            optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "\n",
        "        for step in range(0, args.num_steps):\n",
        "            global_step += 1 * args.num_envs\n",
        "            dones[step] = next_done\n",
        "\n",
        "            # ALGO LOGIC: action logic\n",
        "            with torch.no_grad():\n",
        "                communicated_obs = agent.get_communicated_obs(next_gnn_obs, positions)\n",
        "                action, logprob, entropy, value = agent.get_action_and_value(communicated_obs)\n",
        "\n",
        "            obs[step] = communicated_obs\n",
        "            values[step] = value.flatten()\n",
        "            actions[step] = action\n",
        "            logprobs[step] = logprob\n",
        "\n",
        "            # execute the game and log data\n",
        "            action_array = action.view(args.num_envs, args.num_agents).cpu().numpy()\n",
        "            action_list = [action_array[:, i] for i in range(args.num_agents)]\n",
        "            next_obs_list, reward_list, done, info = envs.step(action_list)\n",
        "\n",
        "            reward = torch.stack(reward_list, dim=1).to(device)\n",
        "            env_rewards = torch.sum(reward, dim=1).cpu().numpy()\n",
        "            episode_returns += env_rewards\n",
        "            episode_lengths += 1\n",
        "\n",
        "            rewards[step] = reward.flatten()\n",
        "\n",
        "            next_obs = torch.stack(next_obs_list, dim=1).to(device) # [num_envs, num_agents, obs_dim]\n",
        "            next_obs = next_obs.view(-1, obs_dim) # [num_envs * num_agents, obs_dim]\n",
        "            next_obs = add_agent_id(next_obs, args.num_envs, args.num_agents, device) # [num_envs * num_agents, obs_dim + num_agents]\n",
        "            next_gnn_obs = next_obs.view(-1, args.num_agents, obs_dim + args.num_agents)\n",
        "\n",
        "            next_done = done.to(device).unsqueeze(1).repeat(1, args.num_agents) # [num_envs, ] => [num_envs, num_agents]\n",
        "            next_done = next_done.flatten() # [num_envs * num_agents, ]\n",
        "\n",
        "            positions = []\n",
        "            for e in range(args.num_envs):\n",
        "                coords = torch.stack([agent.state.pos[e] for agent in envs.agents], dim=0).to(device)  # [n_agents, pos_dim]\n",
        "                positions.append(coords)\n",
        "            positions = torch.stack(positions, dim=0).to(device)  # [-1, num_agents, 2]\n",
        "\n",
        "            episode_ret = []\n",
        "            episode_len = []\n",
        "\n",
        "            for i in range(len(done)):\n",
        "                if done[i]:\n",
        "                    episode_ret.append(episode_returns[i])\n",
        "                    episode_len.append(episode_lengths[i])\n",
        "                    episode_returns[i] = 0\n",
        "                    episode_lengths[i] = 0\n",
        "\n",
        "            # logging episode return and length\n",
        "            if episode_ret and global_step > 100:\n",
        "                print(f\"global_step={global_step}, episodic_return={np.mean(episode_ret)}\")\n",
        "                writer.add_scalar(\"charts/episodic_return\", np.mean(episode_ret), global_step)\n",
        "                writer.add_scalar(\"charts/episodic_length\", np.mean(episode_len), global_step)\n",
        "\n",
        "        # bootstrap value if not done\n",
        "        with torch.no_grad():\n",
        "            next_value = agent.get_value(communicated_obs).reshape(1, -1)\n",
        "            if args.gae:\n",
        "                advantages = torch.zeros_like(rewards).to(device)\n",
        "                lastgaelam = 0\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done.float()\n",
        "                        nextvalues = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1].float()\n",
        "                        nextvalues = values[t + 1]\n",
        "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
        "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "                returns = advantages + values\n",
        "            else:\n",
        "                returns = torch.zeros_like(rewards).to(device)\n",
        "                for t in reversed(range(args.num_steps)):\n",
        "                    if t == args.num_steps - 1:\n",
        "                        nextnonterminal = 1.0 - next_done.float()\n",
        "                        next_return = next_value\n",
        "                    else:\n",
        "                        nextnonterminal = 1.0 - dones[t + 1].float()\n",
        "                        next_return = returns[t + 1]\n",
        "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "                advantages = returns - values\n",
        "\n",
        "        # flatten the batch\n",
        "        b_obs = obs.reshape((-1, args.hidden_dim))\n",
        "        b_logprobs = logprobs.reshape(-1)\n",
        "        b_actions = actions.reshape((-1,1))\n",
        "        b_advantages = advantages.reshape(-1)\n",
        "        b_returns = returns.reshape(-1)\n",
        "        b_values = values.reshape(-1)\n",
        "\n",
        "        # Optimizing the policy and value network\n",
        "        b_inds = np.arange(args.batch_size)\n",
        "        clipfracs = []\n",
        "        for epoch in range(args.update_epochs):\n",
        "            np.random.shuffle(b_inds)\n",
        "            for start in range(0, args.batch_size, args.minibatch_size):\n",
        "                end = start + args.minibatch_size\n",
        "                mb_inds = b_inds[start:end]\n",
        "\n",
        "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "\n",
        "                logratio = newlogprob - b_logprobs[mb_inds]\n",
        "                ratio = logratio.exp()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "                    old_approx_kl = (-logratio).mean()\n",
        "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
        "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "                mb_advantages = b_advantages[mb_inds]\n",
        "                if args.norm_adv:\n",
        "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "                # Policy loss\n",
        "                pg_loss1 = -mb_advantages * ratio\n",
        "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                newvalue = newvalue.view(-1)\n",
        "                if args.clip_vloss:\n",
        "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "                    v_loss = 0.5 * v_loss_max.mean()\n",
        "                else:\n",
        "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "                entropy_loss = entropy.mean()\n",
        "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "\n",
        "            if args.target_kl is not None:\n",
        "                if approx_kl > args.target_kl:\n",
        "                    break\n",
        "\n",
        "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "        var_y = np.var(y_true)\n",
        "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "        # record rewards for plotting purposes\n",
        "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "        if global_step % SAVE_INTERVAL < 1000:\n",
        "            torch.save(agent.actor.state_dict(), f\"{run_name}_policy_{global_step}.pth\")\n",
        "            torch.save(agent.critic.state_dict(), f\"{run_name}_critic_{global_step}.pth\")\n",
        "            print(\"SAVE MODEL in global_step = \", global_step)\n",
        "\n",
        "    torch.save(agent.actor.state_dict(), f\"{run_name}_policy.pth\")\n",
        "    torch.save(agent.critic.state_dict(), f\"{run_name}_critic.pth\")\n",
        "\n",
        "    writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i7j05ZBWnmW",
        "outputId": "7329c0f7-61a3-4928-b9be-46002c103d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=60000, episodic_return=-4.67769193649292\n",
            "SPS: 1574\n",
            "global_step=60600, episodic_return=-0.04767770320177078\n",
            "global_step=61200, episodic_return=-0.043007101863622665\n",
            "global_step=61800, episodic_return=-0.053333912044763565\n",
            "global_step=62400, episodic_return=-0.07168986648321152\n",
            "global_step=63000, episodic_return=-0.060140419751405716\n",
            "global_step=63600, episodic_return=-0.03862128406763077\n",
            "global_step=64200, episodic_return=-0.04448026046156883\n",
            "global_step=64800, episodic_return=-0.048524558544158936\n",
            "global_step=65400, episodic_return=-0.06828081607818604\n",
            "global_step=66000, episodic_return=-0.08017471432685852\n",
            "global_step=66600, episodic_return=-0.0742112323641777\n",
            "global_step=67200, episodic_return=-0.06346938014030457\n",
            "global_step=67800, episodic_return=-0.05459117889404297\n",
            "global_step=68400, episodic_return=-0.05288754031062126\n",
            "global_step=69000, episodic_return=-0.041751086711883545\n",
            "global_step=69600, episodic_return=-0.036984387785196304\n",
            "global_step=70200, episodic_return=-0.03102046251296997\n",
            "global_step=70800, episodic_return=-0.03210094943642616\n",
            "global_step=71400, episodic_return=-0.03742057830095291\n",
            "global_step=72000, episodic_return=-0.027783473953604698\n",
            "global_step=72600, episodic_return=-0.029223036020994186\n",
            "global_step=73200, episodic_return=-0.04652184620499611\n",
            "global_step=73800, episodic_return=-0.05009732395410538\n",
            "global_step=74400, episodic_return=-0.055598240345716476\n",
            "global_step=75000, episodic_return=-0.04871629551053047\n",
            "global_step=75600, episodic_return=-0.04948551207780838\n",
            "global_step=76200, episodic_return=-0.03867381066083908\n",
            "global_step=76800, episodic_return=-0.04202059283852577\n",
            "global_step=77400, episodic_return=-0.04859897494316101\n",
            "global_step=78000, episodic_return=-0.05834265425801277\n",
            "global_step=78600, episodic_return=-0.04524961858987808\n",
            "global_step=79200, episodic_return=-0.04798486828804016\n",
            "global_step=79800, episodic_return=-0.03656641021370888\n",
            "global_step=80400, episodic_return=-0.025986244902014732\n",
            "global_step=81000, episodic_return=-0.030114740133285522\n",
            "global_step=81600, episodic_return=-0.037900447845458984\n",
            "global_step=82200, episodic_return=-0.05570318177342415\n",
            "global_step=82800, episodic_return=-0.05925615131855011\n",
            "global_step=83400, episodic_return=-0.04172965884208679\n",
            "global_step=84000, episodic_return=-0.07012391090393066\n",
            "global_step=84600, episodic_return=-0.07166039198637009\n",
            "global_step=85200, episodic_return=-0.059744223952293396\n",
            "global_step=85800, episodic_return=-0.04672617092728615\n",
            "global_step=86400, episodic_return=-0.04250810667872429\n",
            "global_step=87000, episodic_return=-0.034981999546289444\n",
            "global_step=87600, episodic_return=-0.04160497337579727\n",
            "global_step=88200, episodic_return=-0.042729251086711884\n",
            "global_step=88800, episodic_return=-0.0400468111038208\n",
            "global_step=89400, episodic_return=-0.04290002956986427\n",
            "global_step=90000, episodic_return=-0.05743855610489845\n",
            "global_step=90600, episodic_return=-0.06560131907463074\n",
            "global_step=91200, episodic_return=-0.062079861760139465\n",
            "global_step=91800, episodic_return=-0.03976049646735191\n",
            "global_step=92400, episodic_return=-0.058989256620407104\n",
            "global_step=93000, episodic_return=-0.05784907191991806\n",
            "global_step=93600, episodic_return=-0.044037893414497375\n",
            "global_step=94200, episodic_return=-0.04162291809916496\n",
            "global_step=94800, episodic_return=-0.04033662751317024\n",
            "global_step=95400, episodic_return=-0.028878554701805115\n",
            "global_step=96000, episodic_return=-0.030454978346824646\n",
            "global_step=96600, episodic_return=-0.044047292321920395\n",
            "global_step=97200, episodic_return=-0.045576389878988266\n",
            "global_step=97800, episodic_return=-0.04006293788552284\n",
            "global_step=98400, episodic_return=-0.04312800243496895\n",
            "global_step=99000, episodic_return=-0.048447392880916595\n",
            "global_step=99600, episodic_return=-0.03993501886725426\n",
            "global_step=100200, episodic_return=-0.043741073459386826\n",
            "global_step=100800, episodic_return=-0.05796653777360916\n",
            "global_step=101400, episodic_return=-0.03696897625923157\n",
            "global_step=102000, episodic_return=-0.02718309685587883\n",
            "global_step=102600, episodic_return=-0.039145417511463165\n",
            "global_step=103200, episodic_return=-0.04625913128256798\n",
            "global_step=103800, episodic_return=-0.03786814585328102\n",
            "global_step=104400, episodic_return=-0.025176573544740677\n",
            "global_step=105000, episodic_return=-0.04217913746833801\n",
            "global_step=105600, episodic_return=-0.0659818947315216\n",
            "global_step=106200, episodic_return=-0.04824965447187424\n",
            "global_step=106800, episodic_return=-0.039116960018873215\n",
            "global_step=107400, episodic_return=-0.03384809568524361\n",
            "global_step=108000, episodic_return=-0.03743749484419823\n",
            "global_step=108600, episodic_return=-0.05367279052734375\n",
            "global_step=109200, episodic_return=-0.05711493641138077\n",
            "global_step=109800, episodic_return=-0.04819215089082718\n",
            "global_step=110400, episodic_return=-0.03875330835580826\n",
            "global_step=111000, episodic_return=-0.04026738181710243\n",
            "global_step=111600, episodic_return=-0.05691950395703316\n",
            "global_step=112200, episodic_return=-0.06264210492372513\n",
            "global_step=112800, episodic_return=-0.03450814262032509\n",
            "global_step=113400, episodic_return=-0.030413774773478508\n",
            "global_step=114000, episodic_return=-0.037267543375492096\n",
            "global_step=114600, episodic_return=-0.05152292177081108\n",
            "global_step=115200, episodic_return=-0.036052606999874115\n",
            "global_step=115800, episodic_return=-0.03179480880498886\n",
            "global_step=116400, episodic_return=-0.03206726163625717\n",
            "global_step=117000, episodic_return=-0.022928254678845406\n",
            "global_step=117600, episodic_return=-0.038822442293167114\n",
            "global_step=118200, episodic_return=-0.058723773807287216\n",
            "global_step=118800, episodic_return=-0.0550350584089756\n",
            "global_step=119400, episodic_return=-0.039320673793554306\n",
            "global_step=120000, episodic_return=-0.03441150113940239\n",
            "SPS: 1564\n",
            "global_step=120600, episodic_return=-0.03209724277257919\n",
            "global_step=121200, episodic_return=-0.031613100320100784\n",
            "global_step=121800, episodic_return=-0.04072798788547516\n",
            "global_step=122400, episodic_return=-0.03251781314611435\n",
            "global_step=123000, episodic_return=-0.03002195432782173\n",
            "global_step=123600, episodic_return=-0.04514196515083313\n",
            "global_step=124200, episodic_return=-0.03834330290555954\n",
            "global_step=124800, episodic_return=-0.044441405683755875\n",
            "global_step=125400, episodic_return=-0.03697175905108452\n",
            "global_step=126000, episodic_return=-0.04033961147069931\n",
            "global_step=126600, episodic_return=-0.02862210012972355\n",
            "global_step=127200, episodic_return=-0.03413551300764084\n",
            "global_step=127800, episodic_return=-0.046930816024541855\n",
            "global_step=128400, episodic_return=-0.03341672942042351\n",
            "global_step=129000, episodic_return=-0.030028456822037697\n",
            "global_step=129600, episodic_return=-0.032979100942611694\n",
            "global_step=130200, episodic_return=-0.04404333606362343\n",
            "global_step=130800, episodic_return=-0.050246525555849075\n",
            "global_step=131400, episodic_return=-0.04901978373527527\n",
            "global_step=132000, episodic_return=-0.04027372971177101\n",
            "global_step=132600, episodic_return=-0.04957699030637741\n",
            "global_step=133200, episodic_return=-0.0587419718503952\n",
            "global_step=133800, episodic_return=-0.03691834956407547\n",
            "global_step=134400, episodic_return=-0.027718881145119667\n",
            "global_step=135000, episodic_return=-0.04085003584623337\n",
            "global_step=135600, episodic_return=-0.04588337987661362\n",
            "global_step=136200, episodic_return=-0.0412273034453392\n",
            "global_step=136800, episodic_return=-0.02836601622402668\n",
            "global_step=137400, episodic_return=-0.03971884399652481\n",
            "global_step=138000, episodic_return=-0.052146609872579575\n",
            "global_step=138600, episodic_return=-0.0582742765545845\n",
            "global_step=139200, episodic_return=-0.0495319738984108\n",
            "global_step=139800, episodic_return=-0.0518050454556942\n",
            "global_step=140400, episodic_return=-0.04887939989566803\n",
            "global_step=141000, episodic_return=-0.04844200238585472\n",
            "global_step=141600, episodic_return=-0.05101240053772926\n",
            "global_step=142200, episodic_return=-0.05146314576268196\n",
            "global_step=142800, episodic_return=-0.0406339205801487\n",
            "global_step=143400, episodic_return=-0.045881807804107666\n",
            "global_step=144000, episodic_return=-0.07507486641407013\n",
            "global_step=144600, episodic_return=-0.07104089111089706\n",
            "global_step=145200, episodic_return=-0.06953582912683487\n",
            "global_step=145800, episodic_return=-0.07193204015493393\n",
            "global_step=146400, episodic_return=-0.061399586498737335\n",
            "global_step=147000, episodic_return=-0.05184883251786232\n",
            "global_step=147600, episodic_return=-0.03507431969046593\n",
            "global_step=148200, episodic_return=-0.02996400184929371\n",
            "global_step=148800, episodic_return=-0.03924785554409027\n",
            "global_step=149400, episodic_return=-0.048309795558452606\n",
            "global_step=150000, episodic_return=-0.05163000524044037\n",
            "global_step=150600, episodic_return=-0.05168640613555908\n",
            "global_step=151200, episodic_return=-0.0424220897257328\n",
            "global_step=151800, episodic_return=-0.05149479955434799\n",
            "global_step=152400, episodic_return=-0.05873298645019531\n",
            "global_step=153000, episodic_return=-0.05573546141386032\n",
            "global_step=153600, episodic_return=-0.04831627383828163\n",
            "global_step=154200, episodic_return=-0.046927616000175476\n",
            "global_step=154800, episodic_return=-0.05140625685453415\n",
            "global_step=155400, episodic_return=-0.04269711673259735\n",
            "global_step=156000, episodic_return=-0.051238663494586945\n",
            "global_step=156600, episodic_return=-0.05569634586572647\n",
            "global_step=157200, episodic_return=-0.07726871222257614\n",
            "global_step=157800, episodic_return=-0.05730341747403145\n",
            "global_step=158400, episodic_return=-0.0482352152466774\n",
            "global_step=159000, episodic_return=-0.04105444625020027\n",
            "global_step=159600, episodic_return=-0.03651841729879379\n",
            "global_step=160200, episodic_return=-0.027526728808879852\n",
            "global_step=160800, episodic_return=-0.0420374758541584\n",
            "global_step=161400, episodic_return=-0.04869307950139046\n",
            "global_step=162000, episodic_return=-0.04767272621393204\n",
            "global_step=162600, episodic_return=-0.03182124346494675\n",
            "global_step=163200, episodic_return=-0.03600124269723892\n",
            "global_step=163800, episodic_return=-0.035687655210494995\n",
            "global_step=164400, episodic_return=-0.04238515719771385\n",
            "global_step=165000, episodic_return=-0.05100074037909508\n",
            "global_step=165600, episodic_return=-0.0408063568174839\n",
            "global_step=166200, episodic_return=-0.033474139869213104\n",
            "global_step=166800, episodic_return=-0.029023170471191406\n",
            "global_step=167400, episodic_return=-0.030236270278692245\n",
            "global_step=168000, episodic_return=-0.036987971514463425\n",
            "global_step=168600, episodic_return=-0.03900854289531708\n",
            "global_step=169200, episodic_return=-0.043063171207904816\n",
            "global_step=169800, episodic_return=-0.04031416401267052\n",
            "global_step=170400, episodic_return=-0.03240114077925682\n",
            "global_step=171000, episodic_return=-0.0363905169069767\n",
            "global_step=171600, episodic_return=-0.04338344559073448\n",
            "global_step=172200, episodic_return=-0.038211606442928314\n",
            "global_step=172800, episodic_return=-0.05395998805761337\n",
            "global_step=173400, episodic_return=-0.031640730798244476\n",
            "global_step=174000, episodic_return=-0.018899928778409958\n",
            "global_step=174600, episodic_return=-0.028928369283676147\n",
            "global_step=175200, episodic_return=-0.06528116762638092\n",
            "global_step=175800, episodic_return=-0.08567439764738083\n",
            "global_step=176400, episodic_return=-0.04274225980043411\n",
            "global_step=177000, episodic_return=-0.02220231294631958\n",
            "global_step=177600, episodic_return=-0.039086323231458664\n",
            "global_step=178200, episodic_return=-0.038725078105926514\n",
            "global_step=178800, episodic_return=-0.021293409168720245\n",
            "global_step=179400, episodic_return=-0.034636326134204865\n",
            "global_step=180000, episodic_return=-0.04357171803712845\n",
            "SPS: 1565\n",
            "global_step=180600, episodic_return=-0.03787648677825928\n",
            "global_step=181200, episodic_return=-0.04854820296168327\n",
            "global_step=181800, episodic_return=-0.04449117183685303\n",
            "global_step=182400, episodic_return=-0.056023456156253815\n",
            "global_step=183000, episodic_return=-0.04957456514239311\n",
            "global_step=183600, episodic_return=-0.042557768523693085\n",
            "global_step=184200, episodic_return=-0.03408047929406166\n",
            "global_step=184800, episodic_return=-0.039514973759651184\n",
            "global_step=185400, episodic_return=-0.045832667499780655\n",
            "global_step=186000, episodic_return=-0.044143255800008774\n",
            "global_step=186600, episodic_return=-0.03589770197868347\n",
            "global_step=187200, episodic_return=-0.04220921918749809\n",
            "global_step=187800, episodic_return=-0.03429528325796127\n",
            "global_step=188400, episodic_return=-0.045562125742435455\n",
            "global_step=189000, episodic_return=-0.04671083390712738\n",
            "global_step=189600, episodic_return=-0.04933568462729454\n",
            "global_step=190200, episodic_return=-0.042418040335178375\n",
            "global_step=190800, episodic_return=-0.04365069046616554\n",
            "global_step=191400, episodic_return=-0.029015811160206795\n",
            "global_step=192000, episodic_return=-0.043090518563985825\n",
            "global_step=192600, episodic_return=-0.0455612950026989\n",
            "global_step=193200, episodic_return=-0.03605196997523308\n",
            "global_step=193800, episodic_return=-0.04135189577937126\n",
            "global_step=194400, episodic_return=-0.03498416393995285\n",
            "global_step=195000, episodic_return=-0.02587023377418518\n",
            "global_step=195600, episodic_return=-0.023657383397221565\n",
            "global_step=196200, episodic_return=-0.03425959125161171\n",
            "global_step=196800, episodic_return=-0.03634103015065193\n",
            "global_step=197400, episodic_return=-0.03333159163594246\n",
            "global_step=198000, episodic_return=-0.04155607521533966\n",
            "global_step=198600, episodic_return=-0.04093458130955696\n",
            "global_step=199200, episodic_return=-0.03623172268271446\n",
            "global_step=199800, episodic_return=-0.0349951907992363\n",
            "global_step=200400, episodic_return=-0.025684580206871033\n",
            "global_step=201000, episodic_return=-0.027365900576114655\n",
            "global_step=201600, episodic_return=-0.02655244804918766\n",
            "global_step=202200, episodic_return=-0.029620904475450516\n",
            "global_step=202800, episodic_return=-0.03392099216580391\n",
            "global_step=203400, episodic_return=-0.04718228802084923\n",
            "global_step=204000, episodic_return=-0.04101211577653885\n",
            "global_step=204600, episodic_return=-0.04528617486357689\n",
            "global_step=205200, episodic_return=-0.043938152492046356\n",
            "global_step=205800, episodic_return=-0.03979971632361412\n",
            "global_step=206400, episodic_return=-0.044188424944877625\n",
            "global_step=207000, episodic_return=-0.04011606052517891\n",
            "global_step=207600, episodic_return=-0.03736812621355057\n",
            "global_step=208200, episodic_return=-0.034127261489629745\n",
            "global_step=208800, episodic_return=-0.03710144758224487\n",
            "global_step=209400, episodic_return=-0.04269528016448021\n",
            "global_step=210000, episodic_return=-0.02900274284183979\n",
            "global_step=210600, episodic_return=-0.03891008347272873\n",
            "global_step=211200, episodic_return=-0.026660600677132607\n",
            "global_step=211800, episodic_return=-0.02964366041123867\n",
            "global_step=212400, episodic_return=-0.026386136189103127\n",
            "global_step=213000, episodic_return=-0.024080291390419006\n",
            "global_step=213600, episodic_return=-0.03801407665014267\n",
            "global_step=214200, episodic_return=-0.041055828332901\n",
            "global_step=214800, episodic_return=-0.04441870376467705\n",
            "global_step=215400, episodic_return=-0.03973688930273056\n",
            "global_step=216000, episodic_return=-0.04361344501376152\n",
            "global_step=216600, episodic_return=-0.04548034816980362\n",
            "global_step=217200, episodic_return=-0.04472406953573227\n",
            "global_step=217800, episodic_return=-0.04107445105910301\n",
            "global_step=218400, episodic_return=-0.06778305768966675\n",
            "global_step=219000, episodic_return=-0.06263253092765808\n",
            "global_step=219600, episodic_return=-0.06369388103485107\n",
            "global_step=220200, episodic_return=-0.05060345306992531\n",
            "global_step=220800, episodic_return=-0.05234983563423157\n",
            "global_step=221400, episodic_return=-0.056242141872644424\n",
            "global_step=222000, episodic_return=-0.05580617114901543\n",
            "global_step=222600, episodic_return=-0.06060061231255531\n",
            "global_step=223200, episodic_return=-0.0462849885225296\n",
            "global_step=223800, episodic_return=-0.04378126934170723\n",
            "global_step=224400, episodic_return=-0.044475458562374115\n",
            "global_step=225000, episodic_return=-0.05002940446138382\n",
            "global_step=225600, episodic_return=-0.04729534313082695\n",
            "global_step=226200, episodic_return=-0.03358549624681473\n",
            "global_step=226800, episodic_return=-0.039074622094631195\n",
            "global_step=227400, episodic_return=-0.03155256807804108\n",
            "global_step=228000, episodic_return=-0.03210800513625145\n",
            "global_step=228600, episodic_return=-0.023666279390454292\n",
            "global_step=229200, episodic_return=-0.008008023723959923\n",
            "global_step=229800, episodic_return=-0.041501522064208984\n",
            "global_step=230400, episodic_return=-0.0491931214928627\n",
            "global_step=231000, episodic_return=-0.03634073957800865\n",
            "global_step=231600, episodic_return=-0.03564868122339249\n",
            "global_step=232200, episodic_return=-0.043834444135427475\n",
            "global_step=232800, episodic_return=-0.04579149931669235\n",
            "global_step=233400, episodic_return=-0.03348483890295029\n",
            "global_step=234000, episodic_return=-0.036066684871912\n",
            "global_step=234600, episodic_return=-0.03363051265478134\n",
            "global_step=235200, episodic_return=-0.0404106080532074\n",
            "global_step=235800, episodic_return=-0.04751575365662575\n",
            "global_step=236400, episodic_return=-0.04578281193971634\n",
            "global_step=237000, episodic_return=-0.034513767808675766\n",
            "global_step=237600, episodic_return=-0.04650180786848068\n",
            "global_step=238200, episodic_return=-0.06002330780029297\n",
            "global_step=238800, episodic_return=-0.05501403287053108\n",
            "global_step=239400, episodic_return=-0.02230335958302021\n",
            "global_step=240000, episodic_return=-0.027280660346150398\n",
            "SPS: 1560\n",
            "global_step=240600, episodic_return=-0.02344323880970478\n",
            "global_step=241200, episodic_return=-0.026097571477293968\n",
            "global_step=241800, episodic_return=-0.029780393466353416\n",
            "global_step=242400, episodic_return=-0.04416871443390846\n",
            "global_step=243000, episodic_return=-0.04567008465528488\n",
            "global_step=243600, episodic_return=-0.03697867691516876\n",
            "global_step=244200, episodic_return=-0.038266584277153015\n",
            "global_step=244800, episodic_return=-0.035538218915462494\n",
            "global_step=245400, episodic_return=-0.031043903902173042\n",
            "global_step=246000, episodic_return=-0.026259874925017357\n",
            "global_step=246600, episodic_return=-0.02876484952867031\n",
            "global_step=247200, episodic_return=-0.02826754003763199\n",
            "global_step=247800, episodic_return=-0.03647948428988457\n",
            "global_step=248400, episodic_return=-0.038444723933935165\n",
            "global_step=249000, episodic_return=-0.028933394700288773\n",
            "global_step=249600, episodic_return=-0.03328657150268555\n",
            "global_step=250200, episodic_return=-0.025156166404485703\n",
            "global_step=250800, episodic_return=-0.034798089414834976\n",
            "global_step=251400, episodic_return=-0.03414798900485039\n",
            "global_step=252000, episodic_return=-0.02136451005935669\n",
            "global_step=252600, episodic_return=-0.02934388443827629\n",
            "global_step=253200, episodic_return=-0.03436588868498802\n",
            "global_step=253800, episodic_return=-0.03396623581647873\n",
            "global_step=254400, episodic_return=-0.02792280539870262\n",
            "global_step=255000, episodic_return=-0.020009580999612808\n",
            "global_step=255600, episodic_return=-0.024418920278549194\n",
            "global_step=256200, episodic_return=-0.041864313185214996\n",
            "global_step=256800, episodic_return=-0.046270858496427536\n",
            "global_step=257400, episodic_return=-0.03663227707147598\n",
            "global_step=258000, episodic_return=-0.03833061829209328\n",
            "global_step=258600, episodic_return=-0.05555114895105362\n",
            "global_step=259200, episodic_return=-0.04949216917157173\n",
            "global_step=259800, episodic_return=-0.0275285467505455\n",
            "global_step=260400, episodic_return=-0.03801077976822853\n",
            "global_step=261000, episodic_return=-0.030861295759677887\n",
            "global_step=261600, episodic_return=-0.027441583573818207\n",
            "global_step=262200, episodic_return=-0.05241111293435097\n",
            "global_step=262800, episodic_return=-0.05773881822824478\n",
            "global_step=263400, episodic_return=-0.043556153774261475\n",
            "global_step=264000, episodic_return=-0.03697880357503891\n",
            "global_step=264600, episodic_return=-0.056877098977565765\n",
            "global_step=265200, episodic_return=-0.044246699661016464\n",
            "global_step=265800, episodic_return=-0.041246313601732254\n",
            "global_step=266400, episodic_return=-0.045687511563301086\n",
            "global_step=267000, episodic_return=-0.04035572335124016\n",
            "global_step=267600, episodic_return=-0.029184656217694283\n",
            "global_step=268200, episodic_return=-0.04880470409989357\n",
            "global_step=268800, episodic_return=-0.038837730884552\n",
            "global_step=269400, episodic_return=-0.041623555123806\n",
            "global_step=270000, episodic_return=-0.02488122507929802\n",
            "global_step=270600, episodic_return=-0.023936713114380836\n",
            "global_step=271200, episodic_return=-0.03745513781905174\n",
            "global_step=271800, episodic_return=-0.036302126944065094\n",
            "global_step=272400, episodic_return=-0.03209380432963371\n",
            "global_step=273000, episodic_return=-0.04013806954026222\n",
            "global_step=273600, episodic_return=-0.039585959166288376\n",
            "global_step=274200, episodic_return=-0.040676433593034744\n",
            "global_step=274800, episodic_return=-0.0421050526201725\n",
            "global_step=275400, episodic_return=-0.0335010327398777\n",
            "global_step=276000, episodic_return=-0.04573920741677284\n",
            "global_step=276600, episodic_return=-0.05205265060067177\n",
            "global_step=277200, episodic_return=-0.03505576774477959\n",
            "global_step=277800, episodic_return=-0.040532685816287994\n",
            "global_step=278400, episodic_return=-0.04903086647391319\n",
            "global_step=279000, episodic_return=-0.04574105143547058\n",
            "global_step=279600, episodic_return=-0.02825506590306759\n",
            "global_step=280200, episodic_return=-0.045573361217975616\n",
            "global_step=280800, episodic_return=-0.05180284380912781\n",
            "global_step=281400, episodic_return=-0.031110726296901703\n",
            "global_step=282000, episodic_return=-0.025762757286429405\n",
            "global_step=282600, episodic_return=-0.03174087032675743\n",
            "global_step=283200, episodic_return=-0.04036065936088562\n",
            "global_step=283800, episodic_return=-0.03934549167752266\n",
            "global_step=284400, episodic_return=-0.05355494096875191\n",
            "global_step=285000, episodic_return=-0.061616942286491394\n",
            "global_step=285600, episodic_return=-0.03934882953763008\n",
            "global_step=286200, episodic_return=-0.006020736880600452\n",
            "global_step=286800, episodic_return=-0.01216147467494011\n",
            "global_step=287400, episodic_return=-0.031161367893218994\n",
            "global_step=288000, episodic_return=-0.025631189346313477\n",
            "global_step=288600, episodic_return=-0.024225324392318726\n",
            "global_step=289200, episodic_return=-0.02102474868297577\n",
            "global_step=289800, episodic_return=-0.03570548817515373\n",
            "global_step=290400, episodic_return=-0.03401057422161102\n",
            "global_step=291000, episodic_return=-0.05112151429057121\n",
            "global_step=291600, episodic_return=-0.04764992743730545\n",
            "global_step=292200, episodic_return=-0.03445340692996979\n",
            "global_step=292800, episodic_return=-0.033167846500873566\n",
            "global_step=293400, episodic_return=-0.027536878362298012\n",
            "global_step=294000, episodic_return=-0.02353006601333618\n",
            "global_step=294600, episodic_return=-0.02813154272735119\n",
            "global_step=295200, episodic_return=-0.020179523155093193\n",
            "global_step=295800, episodic_return=-0.03149047866463661\n",
            "global_step=296400, episodic_return=-0.030671142041683197\n",
            "global_step=297000, episodic_return=-0.02703048102557659\n",
            "global_step=297600, episodic_return=-0.028417160734534264\n",
            "global_step=298200, episodic_return=-0.040045611560344696\n",
            "global_step=298800, episodic_return=-0.042868345975875854\n",
            "global_step=299400, episodic_return=-0.027061354368925095\n",
            "global_step=300000, episodic_return=-0.03129969909787178\n",
            "SPS: 1563\n",
            "global_step=300600, episodic_return=-0.02610938251018524\n",
            "global_step=301200, episodic_return=-0.03211236000061035\n",
            "global_step=301800, episodic_return=-0.04213549196720123\n",
            "global_step=302400, episodic_return=-0.030044367536902428\n",
            "global_step=303000, episodic_return=-0.026088088750839233\n",
            "global_step=303600, episodic_return=-0.02628314681351185\n",
            "global_step=304200, episodic_return=-0.040377177298069\n",
            "global_step=304800, episodic_return=-0.03934989124536514\n",
            "global_step=305400, episodic_return=-0.03223004192113876\n",
            "global_step=306000, episodic_return=-0.02478845976293087\n",
            "global_step=306600, episodic_return=-0.026463571935892105\n",
            "global_step=307200, episodic_return=-0.030230261385440826\n",
            "global_step=307800, episodic_return=-0.04416562244296074\n",
            "global_step=308400, episodic_return=-0.030094120651483536\n",
            "global_step=309000, episodic_return=-0.024600857868790627\n",
            "global_step=309600, episodic_return=-0.026233453303575516\n",
            "global_step=310200, episodic_return=-0.025607464835047722\n",
            "global_step=310800, episodic_return=-0.03324197977781296\n",
            "global_step=311400, episodic_return=-0.03528725728392601\n",
            "global_step=312000, episodic_return=-0.021342504769563675\n",
            "global_step=312600, episodic_return=-0.028403816744685173\n",
            "global_step=313200, episodic_return=-0.03272002190351486\n",
            "global_step=313800, episodic_return=-0.04348260164260864\n",
            "global_step=314400, episodic_return=-0.02979181334376335\n",
            "global_step=315000, episodic_return=-0.030664188787341118\n",
            "global_step=315600, episodic_return=-0.02737133391201496\n",
            "global_step=316200, episodic_return=-0.026866862550377846\n",
            "global_step=316800, episodic_return=-0.020183730870485306\n",
            "global_step=317400, episodic_return=-0.026082180440425873\n",
            "global_step=318000, episodic_return=-0.0314781591296196\n",
            "global_step=318600, episodic_return=-0.021650684997439384\n",
            "global_step=319200, episodic_return=-0.013668524101376534\n",
            "global_step=319800, episodic_return=-0.029294325038790703\n",
            "global_step=320400, episodic_return=-0.04005979001522064\n",
            "global_step=321000, episodic_return=-0.03628874942660332\n",
            "global_step=321600, episodic_return=-0.033461976796388626\n",
            "global_step=322200, episodic_return=-0.03399587795138359\n",
            "global_step=322800, episodic_return=-0.03265801817178726\n",
            "global_step=323400, episodic_return=-0.028187453746795654\n",
            "global_step=324000, episodic_return=-0.032765042036771774\n",
            "global_step=324600, episodic_return=-0.02417157217860222\n",
            "global_step=325200, episodic_return=-0.01664937660098076\n",
            "global_step=325800, episodic_return=-0.02163364365696907\n",
            "global_step=326400, episodic_return=-0.01773979514837265\n",
            "global_step=327000, episodic_return=-0.02052699401974678\n",
            "global_step=327600, episodic_return=-0.024546202272176743\n",
            "global_step=328200, episodic_return=-0.03689507767558098\n",
            "global_step=328800, episodic_return=-0.017102627083659172\n",
            "global_step=329400, episodic_return=-0.02043158933520317\n",
            "global_step=330000, episodic_return=-0.027017589658498764\n",
            "global_step=330600, episodic_return=-0.02800256386399269\n",
            "global_step=331200, episodic_return=-0.03669074922800064\n",
            "global_step=331800, episodic_return=-0.02539157308638096\n",
            "global_step=332400, episodic_return=-0.029349781572818756\n",
            "global_step=333000, episodic_return=-0.03185972943902016\n",
            "global_step=333600, episodic_return=-0.03186853229999542\n",
            "global_step=334200, episodic_return=-0.03208111599087715\n",
            "global_step=334800, episodic_return=-0.02491532638669014\n",
            "global_step=335400, episodic_return=-0.015523109585046768\n",
            "global_step=336000, episodic_return=-0.023740744218230247\n",
            "global_step=336600, episodic_return=-0.04438616335391998\n",
            "global_step=337200, episodic_return=-0.03955373913049698\n",
            "global_step=337800, episodic_return=-0.03949473798274994\n",
            "global_step=338400, episodic_return=-0.0318644754588604\n",
            "global_step=339000, episodic_return=-0.04256345331668854\n",
            "global_step=339600, episodic_return=-0.05461054667830467\n",
            "global_step=340200, episodic_return=-0.05622420459985733\n",
            "global_step=340800, episodic_return=-0.04855586960911751\n",
            "global_step=341400, episodic_return=-0.03522573411464691\n",
            "global_step=342000, episodic_return=-0.046181194484233856\n",
            "global_step=342600, episodic_return=-0.04263130947947502\n",
            "global_step=343200, episodic_return=-0.031866952776908875\n",
            "global_step=343800, episodic_return=-0.038842037320137024\n",
            "global_step=344400, episodic_return=-0.04163705185055733\n",
            "global_step=345000, episodic_return=-0.02638140879571438\n",
            "global_step=345600, episodic_return=-0.019992230460047722\n",
            "global_step=346200, episodic_return=-0.013152316212654114\n",
            "global_step=346800, episodic_return=-0.025614788755774498\n",
            "global_step=347400, episodic_return=-0.027722012251615524\n",
            "global_step=348000, episodic_return=-0.030328713357448578\n",
            "global_step=348600, episodic_return=-0.029034588485956192\n",
            "global_step=349200, episodic_return=-0.026146776974201202\n",
            "global_step=349800, episodic_return=-0.01956946589052677\n",
            "global_step=350400, episodic_return=-0.024540407583117485\n",
            "global_step=351000, episodic_return=-0.01850162260234356\n",
            "global_step=351600, episodic_return=-0.02983766794204712\n",
            "global_step=352200, episodic_return=-0.042257074266672134\n",
            "global_step=352800, episodic_return=-0.03919898346066475\n",
            "global_step=353400, episodic_return=-0.031049372628331184\n",
            "global_step=354000, episodic_return=-0.016607560217380524\n",
            "global_step=354600, episodic_return=-0.014775236137211323\n",
            "global_step=355200, episodic_return=-0.012829110957682133\n",
            "global_step=355800, episodic_return=-0.01412719115614891\n",
            "global_step=356400, episodic_return=-0.02862096205353737\n",
            "global_step=357000, episodic_return=-0.03448953479528427\n",
            "global_step=357600, episodic_return=-0.030029576271772385\n",
            "global_step=358200, episodic_return=-0.023344548419117928\n",
            "global_step=358800, episodic_return=-0.019580813124775887\n",
            "global_step=359400, episodic_return=-0.028386510908603668\n",
            "global_step=360000, episodic_return=-0.030662650242447853\n",
            "SPS: 1567\n",
            "global_step=360600, episodic_return=-0.02259696088731289\n",
            "global_step=361200, episodic_return=-0.007027002051472664\n",
            "global_step=361800, episodic_return=-0.011624577455222607\n",
            "global_step=362400, episodic_return=-0.016045650467276573\n",
            "global_step=363000, episodic_return=-0.01740642823278904\n",
            "global_step=363600, episodic_return=-0.019603412598371506\n",
            "global_step=364200, episodic_return=-0.03225449100136757\n",
            "global_step=364800, episodic_return=-0.037882715463638306\n",
            "global_step=365400, episodic_return=-0.0378069169819355\n",
            "global_step=366000, episodic_return=-0.02910195104777813\n",
            "global_step=366600, episodic_return=-0.030997874215245247\n",
            "global_step=367200, episodic_return=-0.028785495087504387\n",
            "global_step=367800, episodic_return=-0.021501431241631508\n",
            "global_step=368400, episodic_return=-0.015759360045194626\n",
            "global_step=369000, episodic_return=-0.025828344747424126\n",
            "global_step=369600, episodic_return=-0.029243608936667442\n",
            "global_step=370200, episodic_return=-0.025364480912685394\n",
            "global_step=370800, episodic_return=-0.03431936725974083\n",
            "global_step=371400, episodic_return=-0.02454591542482376\n",
            "global_step=372000, episodic_return=-0.027518538758158684\n",
            "global_step=372600, episodic_return=-0.028637800365686417\n",
            "global_step=373200, episodic_return=-0.033580709248781204\n",
            "global_step=373800, episodic_return=-0.022436343133449554\n",
            "global_step=374400, episodic_return=-0.024319512769579887\n",
            "global_step=375000, episodic_return=-0.019137971103191376\n",
            "global_step=375600, episodic_return=-0.03208312764763832\n",
            "global_step=376200, episodic_return=-0.022266868501901627\n",
            "global_step=376800, episodic_return=-0.019764460623264313\n",
            "global_step=377400, episodic_return=-0.021927980706095695\n",
            "global_step=378000, episodic_return=-0.03301572427153587\n",
            "global_step=378600, episodic_return=-0.04333527386188507\n",
            "global_step=379200, episodic_return=-0.0418117456138134\n",
            "global_step=379800, episodic_return=-0.030627403408288956\n",
            "global_step=380400, episodic_return=-0.014541763812303543\n",
            "global_step=381000, episodic_return=-0.02255801111459732\n",
            "global_step=381600, episodic_return=-0.021882856264710426\n",
            "global_step=382200, episodic_return=-0.027406444773077965\n",
            "global_step=382800, episodic_return=-0.038611844182014465\n",
            "global_step=383400, episodic_return=-0.03477059677243233\n",
            "global_step=384000, episodic_return=-0.027399342507123947\n",
            "global_step=384600, episodic_return=-0.03781693056225777\n",
            "global_step=385200, episodic_return=-0.04382706433534622\n",
            "global_step=385800, episodic_return=-0.03574899956583977\n",
            "global_step=386400, episodic_return=-0.014234156347811222\n",
            "global_step=387000, episodic_return=-0.020749974995851517\n",
            "global_step=387600, episodic_return=-0.027737287804484367\n",
            "global_step=388200, episodic_return=-0.028338102623820305\n",
            "global_step=388800, episodic_return=-0.027883555740118027\n",
            "global_step=389400, episodic_return=-0.03514246270060539\n",
            "global_step=390000, episodic_return=-0.03588956221938133\n",
            "global_step=390600, episodic_return=-0.03144389018416405\n",
            "global_step=391200, episodic_return=-0.029387187212705612\n",
            "global_step=391800, episodic_return=-0.040530044585466385\n",
            "global_step=392400, episodic_return=-0.027097530663013458\n",
            "global_step=393000, episodic_return=-0.027931105345487595\n",
            "global_step=393600, episodic_return=-0.015284421853721142\n",
            "global_step=394200, episodic_return=-0.01680848002433777\n",
            "global_step=394800, episodic_return=-0.026873193681240082\n",
            "global_step=395400, episodic_return=-0.02794789709150791\n",
            "global_step=396000, episodic_return=-0.0258069708943367\n",
            "global_step=396600, episodic_return=-0.02063390426337719\n",
            "global_step=397200, episodic_return=-0.021866923198103905\n",
            "global_step=397800, episodic_return=-0.021523931995034218\n",
            "global_step=398400, episodic_return=-0.019306423142552376\n",
            "global_step=399000, episodic_return=-0.03404196351766586\n",
            "global_step=399600, episodic_return=-0.02228674106299877\n",
            "global_step=400200, episodic_return=-0.013482844457030296\n",
            "global_step=400800, episodic_return=-0.011326697655022144\n",
            "global_step=401400, episodic_return=-0.025068560615181923\n",
            "global_step=402000, episodic_return=-0.02593722939491272\n",
            "global_step=402600, episodic_return=-0.03171933442354202\n",
            "global_step=403200, episodic_return=-0.023496437817811966\n",
            "global_step=403800, episodic_return=-0.04150908812880516\n",
            "global_step=404400, episodic_return=-0.03220808506011963\n",
            "global_step=405000, episodic_return=-0.0332963690161705\n",
            "global_step=405600, episodic_return=-0.020920652896165848\n",
            "global_step=406200, episodic_return=-0.030478211119771004\n",
            "global_step=406800, episodic_return=-0.03092053160071373\n",
            "global_step=407400, episodic_return=-0.02015767991542816\n",
            "global_step=408000, episodic_return=-0.029135532677173615\n",
            "global_step=408600, episodic_return=-0.02524302527308464\n",
            "global_step=409200, episodic_return=-0.021840820088982582\n",
            "global_step=409800, episodic_return=-0.033234693109989166\n",
            "global_step=410400, episodic_return=-0.037519726902246475\n",
            "global_step=411000, episodic_return=-0.040294092148542404\n",
            "global_step=411600, episodic_return=-0.04207570478320122\n",
            "global_step=412200, episodic_return=-0.04035705700516701\n",
            "global_step=412800, episodic_return=-0.024566322565078735\n",
            "global_step=413400, episodic_return=-0.02609301544725895\n",
            "global_step=414000, episodic_return=-0.02295064926147461\n",
            "global_step=414600, episodic_return=-0.025950614362955093\n",
            "global_step=415200, episodic_return=-0.03401026874780655\n",
            "global_step=415800, episodic_return=-0.02486150898039341\n",
            "global_step=416400, episodic_return=-0.012966770678758621\n",
            "global_step=417000, episodic_return=-0.026805752888321877\n",
            "global_step=417600, episodic_return=-0.02777360938489437\n",
            "global_step=418200, episodic_return=-0.024299200624227524\n",
            "global_step=418800, episodic_return=-0.024691730737686157\n",
            "global_step=419400, episodic_return=-0.038610368967056274\n",
            "global_step=420000, episodic_return=-0.04720150679349899\n",
            "SPS: 1571\n",
            "global_step=420600, episodic_return=-0.03883606940507889\n",
            "global_step=421200, episodic_return=-0.04420683532953262\n",
            "global_step=421800, episodic_return=-0.0321684405207634\n",
            "global_step=422400, episodic_return=-0.022505665197968483\n",
            "global_step=423000, episodic_return=-0.008691349998116493\n",
            "global_step=423600, episodic_return=-0.0151235181838274\n",
            "global_step=424200, episodic_return=-0.021495221182703972\n",
            "global_step=424800, episodic_return=-0.02705015242099762\n",
            "global_step=425400, episodic_return=-0.035043321549892426\n",
            "global_step=426000, episodic_return=-0.03386527672410011\n",
            "global_step=426600, episodic_return=-0.04225791245698929\n",
            "global_step=427200, episodic_return=-0.021419968456029892\n",
            "global_step=427800, episodic_return=-0.014924249611794949\n",
            "global_step=428400, episodic_return=-0.011912345886230469\n",
            "global_step=429000, episodic_return=-0.020153548568487167\n",
            "global_step=429600, episodic_return=-0.04197242483496666\n",
            "global_step=430200, episodic_return=-0.04227009788155556\n",
            "global_step=430800, episodic_return=-0.029717639088630676\n",
            "global_step=431400, episodic_return=-0.025929471477866173\n",
            "global_step=432000, episodic_return=-0.028223441913723946\n",
            "global_step=432600, episodic_return=-0.016777068376541138\n",
            "global_step=433200, episodic_return=-0.022089118137955666\n",
            "global_step=433800, episodic_return=-0.025802738964557648\n",
            "global_step=434400, episodic_return=-0.031734663993120193\n",
            "global_step=435000, episodic_return=-0.03866855800151825\n",
            "global_step=435600, episodic_return=-0.03935369476675987\n",
            "global_step=436200, episodic_return=-0.038633160293102264\n",
            "global_step=436800, episodic_return=-0.019147124141454697\n",
            "global_step=437400, episodic_return=-0.025777537375688553\n",
            "global_step=438000, episodic_return=-0.028631890192627907\n",
            "global_step=438600, episodic_return=-0.032557226717472076\n",
            "global_step=439200, episodic_return=-0.021266864612698555\n",
            "global_step=439800, episodic_return=-0.019094886258244514\n",
            "global_step=440400, episodic_return=-0.018621336668729782\n",
            "global_step=441000, episodic_return=-0.043446946889162064\n",
            "global_step=441600, episodic_return=-0.034456197172403336\n",
            "global_step=442200, episodic_return=-0.03286324068903923\n",
            "global_step=442800, episodic_return=-0.015776997432112694\n",
            "global_step=443400, episodic_return=-0.01317611150443554\n",
            "global_step=444000, episodic_return=-0.02176794409751892\n",
            "global_step=444600, episodic_return=-0.027793683111667633\n",
            "global_step=445200, episodic_return=-0.02449645660817623\n",
            "global_step=445800, episodic_return=-0.023800194263458252\n",
            "global_step=446400, episodic_return=-0.02117357775568962\n",
            "global_step=447000, episodic_return=-0.02162240631878376\n",
            "global_step=447600, episodic_return=-0.026120919734239578\n",
            "global_step=448200, episodic_return=-0.015551403164863586\n",
            "global_step=448800, episodic_return=-0.0343284085392952\n",
            "global_step=449400, episodic_return=-0.032200247049331665\n",
            "global_step=450000, episodic_return=-0.01818123646080494\n",
            "global_step=450600, episodic_return=-0.015725433826446533\n",
            "global_step=451200, episodic_return=-0.021488936617970467\n",
            "global_step=451800, episodic_return=-0.016833104193210602\n",
            "global_step=452400, episodic_return=-0.01982475072145462\n",
            "global_step=453000, episodic_return=-0.02484879083931446\n",
            "global_step=453600, episodic_return=-0.026471586897969246\n",
            "global_step=454200, episodic_return=-0.025154173374176025\n",
            "global_step=454800, episodic_return=-0.017635516822338104\n",
            "global_step=455400, episodic_return=-0.016601664945483208\n",
            "global_step=456000, episodic_return=-0.016388412564992905\n",
            "global_step=456600, episodic_return=-0.018332213163375854\n",
            "global_step=457200, episodic_return=-0.021320683881640434\n",
            "global_step=457800, episodic_return=-0.02305181510746479\n",
            "global_step=458400, episodic_return=-0.03832707554101944\n",
            "global_step=459000, episodic_return=-0.04106345400214195\n",
            "global_step=459600, episodic_return=-0.034753214567899704\n",
            "global_step=460200, episodic_return=-0.032124247401952744\n",
            "global_step=460800, episodic_return=-0.017566915601491928\n",
            "global_step=461400, episodic_return=-0.01677270233631134\n",
            "global_step=462000, episodic_return=-0.0211404487490654\n",
            "global_step=462600, episodic_return=-0.024483809247612953\n",
            "global_step=463200, episodic_return=-0.018212970346212387\n",
            "global_step=463800, episodic_return=-0.0266891997307539\n",
            "global_step=464400, episodic_return=-0.026628131046891212\n",
            "global_step=465000, episodic_return=-0.030110429972410202\n",
            "global_step=465600, episodic_return=-0.03276698291301727\n",
            "global_step=466200, episodic_return=-0.038441695272922516\n",
            "global_step=466800, episodic_return=-0.0479903481900692\n",
            "global_step=467400, episodic_return=-0.023883486166596413\n",
            "global_step=468000, episodic_return=-0.02463136985898018\n",
            "global_step=468600, episodic_return=-0.03303147479891777\n",
            "global_step=469200, episodic_return=-0.03224524110555649\n",
            "global_step=469800, episodic_return=-0.020826702937483788\n",
            "global_step=470400, episodic_return=-0.02783738449215889\n",
            "global_step=471000, episodic_return=-0.026420358568429947\n",
            "global_step=471600, episodic_return=-0.03105347603559494\n",
            "global_step=472200, episodic_return=-0.021795332431793213\n",
            "global_step=472800, episodic_return=-0.021688906475901604\n",
            "global_step=473400, episodic_return=-0.027201615273952484\n",
            "global_step=474000, episodic_return=-0.033111345022916794\n",
            "global_step=474600, episodic_return=-0.03435402363538742\n",
            "global_step=475200, episodic_return=-0.029222335666418076\n",
            "global_step=475800, episodic_return=-0.030497808009386063\n",
            "global_step=476400, episodic_return=-0.03078148514032364\n",
            "global_step=477000, episodic_return=-0.023042574524879456\n",
            "global_step=477600, episodic_return=-0.025514010339975357\n",
            "global_step=478200, episodic_return=-0.023886295035481453\n",
            "global_step=478800, episodic_return=-0.03583359345793724\n",
            "global_step=479400, episodic_return=-0.024358555674552917\n",
            "global_step=480000, episodic_return=-0.011283505707979202\n",
            "SPS: 1570\n",
            "global_step=480600, episodic_return=-0.015148604288697243\n",
            "global_step=481200, episodic_return=-0.02086838334798813\n",
            "global_step=481800, episodic_return=-0.026592370122671127\n",
            "global_step=482400, episodic_return=-0.031985919922590256\n",
            "global_step=483000, episodic_return=-0.031540241092443466\n",
            "global_step=483600, episodic_return=-0.0389181524515152\n",
            "global_step=484200, episodic_return=-0.04478895664215088\n",
            "global_step=484800, episodic_return=-0.03726387768983841\n",
            "global_step=485400, episodic_return=-0.023717807605862617\n",
            "global_step=486000, episodic_return=-0.023498916998505592\n",
            "global_step=486600, episodic_return=-0.022310806438326836\n",
            "global_step=487200, episodic_return=-0.019533906131982803\n",
            "global_step=487800, episodic_return=-0.023229841142892838\n",
            "global_step=488400, episodic_return=-0.017524534836411476\n",
            "global_step=489000, episodic_return=-0.012215513736009598\n",
            "global_step=489600, episodic_return=-0.015836123377084732\n",
            "global_step=490200, episodic_return=-0.03205443173646927\n",
            "global_step=490800, episodic_return=-0.02535991743206978\n",
            "global_step=491400, episodic_return=-0.014947342686355114\n",
            "global_step=492000, episodic_return=-0.017399581149220467\n",
            "global_step=492600, episodic_return=-0.02446797303855419\n",
            "global_step=493200, episodic_return=-0.024013901129364967\n",
            "global_step=493800, episodic_return=-0.014944936148822308\n",
            "global_step=494400, episodic_return=-0.02794245444238186\n",
            "global_step=495000, episodic_return=-0.023988371714949608\n",
            "global_step=495600, episodic_return=-0.02388548105955124\n",
            "global_step=496200, episodic_return=-0.018766412511467934\n",
            "global_step=496800, episodic_return=-0.034208107739686966\n",
            "global_step=497400, episodic_return=-0.0342564694583416\n",
            "global_step=498000, episodic_return=-0.03228892758488655\n",
            "global_step=498600, episodic_return=-0.020004834979772568\n",
            "global_step=499200, episodic_return=-0.01980423927307129\n",
            "global_step=499800, episodic_return=-0.024350222200155258\n",
            "global_step=500400, episodic_return=-0.02371389791369438\n",
            "global_step=501000, episodic_return=-0.026648299768567085\n",
            "global_step=501600, episodic_return=-0.030996322631835938\n",
            "global_step=502200, episodic_return=-0.03719090297818184\n",
            "global_step=502800, episodic_return=-0.03814731538295746\n",
            "global_step=503400, episodic_return=-0.038293302059173584\n",
            "global_step=504000, episodic_return=-0.02380501478910446\n",
            "global_step=504600, episodic_return=-0.025111602619290352\n",
            "global_step=505200, episodic_return=-0.027807271108031273\n",
            "global_step=505800, episodic_return=-0.022365402430295944\n",
            "global_step=506400, episodic_return=-0.022081241011619568\n",
            "global_step=507000, episodic_return=-0.019497346132993698\n",
            "global_step=507600, episodic_return=-0.03487297147512436\n",
            "global_step=508200, episodic_return=-0.029736073687672615\n",
            "global_step=508800, episodic_return=-0.03242161497473717\n",
            "global_step=509400, episodic_return=-0.02380220592021942\n",
            "global_step=510000, episodic_return=-0.026051385328173637\n",
            "global_step=510600, episodic_return=-0.035351697355508804\n",
            "global_step=511200, episodic_return=-0.0198269821703434\n",
            "global_step=511800, episodic_return=-0.027147678658366203\n",
            "global_step=512400, episodic_return=-0.035981062799692154\n",
            "global_step=513000, episodic_return=-0.029344230890274048\n",
            "global_step=513600, episodic_return=-0.0403657965362072\n",
            "global_step=514200, episodic_return=-0.03102751076221466\n",
            "global_step=514800, episodic_return=-0.020797058939933777\n",
            "global_step=515400, episodic_return=-0.0208571944385767\n",
            "global_step=516000, episodic_return=-0.013591254130005836\n",
            "global_step=516600, episodic_return=-0.011270850896835327\n",
            "global_step=517200, episodic_return=-0.013688065111637115\n",
            "global_step=517800, episodic_return=-0.015438477508723736\n",
            "global_step=518400, episodic_return=-0.020374787971377373\n",
            "global_step=519000, episodic_return=-0.031434353440999985\n",
            "global_step=519600, episodic_return=-0.02981570176780224\n",
            "global_step=520200, episodic_return=-0.024777600541710854\n",
            "global_step=520800, episodic_return=-0.026047654449939728\n",
            "global_step=521400, episodic_return=-0.03841296583414078\n",
            "global_step=522000, episodic_return=-0.036792296916246414\n",
            "global_step=522600, episodic_return=-0.034445870667696\n",
            "global_step=523200, episodic_return=-0.026330281049013138\n",
            "global_step=523800, episodic_return=-0.017920373007655144\n",
            "global_step=524400, episodic_return=-0.014829401858150959\n",
            "global_step=525000, episodic_return=-0.016017887741327286\n",
            "global_step=525600, episodic_return=-0.00865407194942236\n",
            "global_step=526200, episodic_return=-0.0245702862739563\n",
            "global_step=526800, episodic_return=-0.033565204590559006\n",
            "global_step=527400, episodic_return=-0.04696105793118477\n",
            "global_step=528000, episodic_return=-0.04461154341697693\n",
            "global_step=528600, episodic_return=-0.045415401458740234\n",
            "global_step=529200, episodic_return=-0.03836987167596817\n",
            "global_step=529800, episodic_return=-0.030232416465878487\n",
            "global_step=530400, episodic_return=-0.021824799478054047\n",
            "global_step=531000, episodic_return=-0.019014043733477592\n",
            "global_step=531600, episodic_return=-0.021217072382569313\n",
            "global_step=532200, episodic_return=-0.01950814761221409\n",
            "global_step=532800, episodic_return=-0.012333102524280548\n",
            "global_step=533400, episodic_return=-0.00810597836971283\n",
            "global_step=534000, episodic_return=-0.015347185544669628\n",
            "global_step=534600, episodic_return=-0.01624816097319126\n",
            "global_step=535200, episodic_return=-0.02266814187169075\n",
            "global_step=535800, episodic_return=-0.04178807884454727\n",
            "global_step=536400, episodic_return=-0.041689399629831314\n",
            "global_step=537000, episodic_return=-0.03406284376978874\n",
            "global_step=537600, episodic_return=-0.0372127965092659\n",
            "global_step=538200, episodic_return=-0.03729265183210373\n",
            "global_step=538800, episodic_return=-0.028243081644177437\n",
            "global_step=539400, episodic_return=-0.02263532392680645\n",
            "global_step=540000, episodic_return=-0.006252081133425236\n",
            "SPS: 1567\n",
            "global_step=540600, episodic_return=-0.006754356902092695\n",
            "global_step=541200, episodic_return=-0.00878931488841772\n",
            "global_step=541800, episodic_return=-0.019657157361507416\n",
            "global_step=542400, episodic_return=-0.02099316008388996\n",
            "global_step=543000, episodic_return=-0.037714388221502304\n",
            "global_step=543600, episodic_return=-0.019815411418676376\n",
            "global_step=544200, episodic_return=-0.01943318359553814\n",
            "global_step=544800, episodic_return=-0.0377650260925293\n",
            "global_step=545400, episodic_return=-0.03786667436361313\n",
            "global_step=546000, episodic_return=-0.01687016151845455\n",
            "global_step=546600, episodic_return=-0.012575521133840084\n",
            "global_step=547200, episodic_return=-0.013211937621235847\n",
            "global_step=547800, episodic_return=-0.0177763719111681\n",
            "global_step=548400, episodic_return=-0.018116608262062073\n",
            "global_step=549000, episodic_return=-0.023030264303088188\n",
            "global_step=549600, episodic_return=-0.01875443384051323\n",
            "global_step=550200, episodic_return=-0.015682019293308258\n",
            "global_step=550800, episodic_return=-0.011083532124757767\n",
            "global_step=551400, episodic_return=-0.013215497136116028\n",
            "global_step=552000, episodic_return=-0.01957103982567787\n",
            "global_step=552600, episodic_return=-0.019802652299404144\n",
            "global_step=553200, episodic_return=-0.024849528446793556\n",
            "global_step=553800, episodic_return=-0.018443500623106956\n",
            "global_step=554400, episodic_return=-0.024414796382188797\n",
            "global_step=555000, episodic_return=-0.022620538249611855\n",
            "global_step=555600, episodic_return=-0.02135007083415985\n",
            "global_step=556200, episodic_return=-0.0242965929210186\n",
            "global_step=556800, episodic_return=-0.024222198873758316\n",
            "global_step=557400, episodic_return=-0.024365762248635292\n",
            "global_step=558000, episodic_return=-0.026936670765280724\n",
            "global_step=558600, episodic_return=-0.027898184955120087\n",
            "global_step=559200, episodic_return=-0.026433255523443222\n",
            "global_step=559800, episodic_return=-0.026289358735084534\n",
            "global_step=560400, episodic_return=-0.02340981550514698\n",
            "global_step=561000, episodic_return=-0.020750386640429497\n",
            "global_step=561600, episodic_return=-0.01928708516061306\n",
            "global_step=562200, episodic_return=-0.02338544651865959\n",
            "global_step=562800, episodic_return=-0.019176499918103218\n",
            "global_step=563400, episodic_return=-0.023105457425117493\n",
            "global_step=564000, episodic_return=-0.018204890191555023\n",
            "global_step=564600, episodic_return=-0.03658393397927284\n",
            "global_step=565200, episodic_return=-0.03403054550290108\n",
            "global_step=565800, episodic_return=-0.03213096782565117\n",
            "global_step=566400, episodic_return=-0.020654767751693726\n",
            "global_step=567000, episodic_return=-0.01423819549381733\n",
            "global_step=567600, episodic_return=-0.02651597000658512\n",
            "global_step=568200, episodic_return=-0.0337182879447937\n",
            "global_step=568800, episodic_return=-0.04123977571725845\n",
            "global_step=569400, episodic_return=-0.02544887736439705\n",
            "global_step=570000, episodic_return=-0.023320170119404793\n",
            "global_step=570600, episodic_return=-0.011206559836864471\n",
            "global_step=571200, episodic_return=-0.010808833874762058\n",
            "global_step=571800, episodic_return=-0.014464061707258224\n",
            "global_step=572400, episodic_return=-0.021116742864251137\n",
            "global_step=573000, episodic_return=-0.02137884870171547\n",
            "global_step=573600, episodic_return=-0.017216969281435013\n",
            "global_step=574200, episodic_return=-0.012814688496291637\n",
            "global_step=574800, episodic_return=-0.011669063940644264\n",
            "global_step=575400, episodic_return=-0.012930121272802353\n",
            "global_step=576000, episodic_return=-0.012087891809642315\n",
            "global_step=576600, episodic_return=-0.023749610409140587\n",
            "global_step=577200, episodic_return=-0.02058306150138378\n",
            "global_step=577800, episodic_return=-0.018605155870318413\n",
            "global_step=578400, episodic_return=-0.01982843317091465\n",
            "global_step=579000, episodic_return=-0.024854205548763275\n",
            "global_step=579600, episodic_return=-0.022361479699611664\n",
            "global_step=580200, episodic_return=-0.010383923538029194\n",
            "global_step=580800, episodic_return=-0.014492295682430267\n",
            "global_step=581400, episodic_return=-0.02311773970723152\n",
            "global_step=582000, episodic_return=-0.037942204624414444\n",
            "global_step=582600, episodic_return=-0.028422260656952858\n",
            "global_step=583200, episodic_return=-0.0258759967982769\n",
            "global_step=583800, episodic_return=-0.019892200827598572\n",
            "global_step=584400, episodic_return=-0.030401179566979408\n",
            "global_step=585000, episodic_return=-0.03676848113536835\n",
            "global_step=585600, episodic_return=-0.041605301201343536\n",
            "global_step=586200, episodic_return=-0.021140342578291893\n",
            "global_step=586800, episodic_return=-0.015197032131254673\n",
            "global_step=587400, episodic_return=-0.008790824562311172\n",
            "global_step=588000, episodic_return=-0.012738743796944618\n",
            "global_step=588600, episodic_return=-0.019665254279971123\n",
            "global_step=589200, episodic_return=-0.018738236278295517\n",
            "global_step=589800, episodic_return=-0.010071275755763054\n",
            "global_step=590400, episodic_return=-0.010257044807076454\n",
            "global_step=591000, episodic_return=-0.009140679612755775\n",
            "global_step=591600, episodic_return=-0.00684535363689065\n",
            "global_step=592200, episodic_return=-0.004193078726530075\n",
            "global_step=592800, episodic_return=-0.008111794479191303\n",
            "global_step=593400, episodic_return=-0.01746843010187149\n",
            "global_step=594000, episodic_return=-0.014175338670611382\n",
            "global_step=594600, episodic_return=-0.009823872707784176\n",
            "global_step=595200, episodic_return=-0.024904532358050346\n",
            "global_step=595800, episodic_return=-0.030861157923936844\n",
            "global_step=596400, episodic_return=-0.013662419281899929\n",
            "global_step=597000, episodic_return=-0.01727556623518467\n",
            "global_step=597600, episodic_return=-0.019051404669880867\n",
            "global_step=598200, episodic_return=-0.026750415563583374\n",
            "global_step=598800, episodic_return=-0.02787037193775177\n",
            "global_step=599400, episodic_return=-0.019571108743548393\n",
            "global_step=600000, episodic_return=-0.022112572565674782\n",
            "SPS: 1566\n",
            "global_step=600600, episodic_return=-0.0154420156031847\n",
            "global_step=601200, episodic_return=-0.012657122686505318\n",
            "global_step=601800, episodic_return=-0.006551807280629873\n",
            "global_step=602400, episodic_return=-0.00728473998606205\n",
            "global_step=603000, episodic_return=-0.01938534341752529\n",
            "global_step=603600, episodic_return=-0.021788517013192177\n",
            "global_step=604200, episodic_return=-0.024879315868020058\n",
            "global_step=604800, episodic_return=-0.01962762139737606\n",
            "global_step=605400, episodic_return=-0.02049100585281849\n",
            "global_step=606000, episodic_return=-0.015323540195822716\n",
            "global_step=606600, episodic_return=-0.004369450733065605\n",
            "global_step=607200, episodic_return=-0.0025944141671061516\n",
            "global_step=607800, episodic_return=-0.00964127853512764\n",
            "global_step=608400, episodic_return=-0.019983908161520958\n",
            "global_step=609000, episodic_return=-0.02936159074306488\n",
            "global_step=609600, episodic_return=-0.027226397767663002\n",
            "global_step=610200, episodic_return=-0.019011884927749634\n",
            "global_step=610800, episodic_return=-0.016140731051564217\n",
            "global_step=611400, episodic_return=-0.026075510308146477\n",
            "global_step=612000, episodic_return=-0.017143147066235542\n",
            "global_step=612600, episodic_return=-0.015324048697948456\n",
            "global_step=613200, episodic_return=-0.006886365357786417\n",
            "global_step=613800, episodic_return=-0.01821819320321083\n",
            "global_step=614400, episodic_return=-0.027269519865512848\n",
            "global_step=615000, episodic_return=-0.02462911047041416\n",
            "global_step=615600, episodic_return=-0.019848724827170372\n",
            "global_step=616200, episodic_return=-0.011454155668616295\n",
            "global_step=616800, episodic_return=-0.019701767712831497\n",
            "global_step=617400, episodic_return=-0.02132643386721611\n",
            "global_step=618000, episodic_return=-0.017426474019885063\n",
            "global_step=618600, episodic_return=-0.017760302871465683\n",
            "global_step=619200, episodic_return=-0.00918430183082819\n",
            "global_step=619800, episodic_return=-0.020089467987418175\n",
            "global_step=620400, episodic_return=-0.024742567911744118\n",
            "global_step=621000, episodic_return=-0.013061124831438065\n",
            "global_step=621600, episodic_return=-0.008052877150475979\n",
            "global_step=622200, episodic_return=-0.008072834461927414\n",
            "global_step=622800, episodic_return=-0.009778296574950218\n",
            "global_step=623400, episodic_return=-0.01664002053439617\n",
            "global_step=624000, episodic_return=-0.026924051344394684\n",
            "global_step=624600, episodic_return=-0.030790094286203384\n",
            "global_step=625200, episodic_return=-0.03752044215798378\n",
            "global_step=625800, episodic_return=-0.0288861021399498\n",
            "global_step=626400, episodic_return=-0.01632171869277954\n",
            "global_step=627000, episodic_return=-0.019948003813624382\n",
            "global_step=627600, episodic_return=-0.015573992393910885\n",
            "global_step=628200, episodic_return=-0.02524096518754959\n",
            "global_step=628800, episodic_return=-0.030727803707122803\n",
            "global_step=629400, episodic_return=-0.025751711800694466\n",
            "global_step=630000, episodic_return=-0.01910329796373844\n",
            "global_step=630600, episodic_return=-0.023016296327114105\n",
            "global_step=631200, episodic_return=-0.023837191984057426\n",
            "global_step=631800, episodic_return=-0.020012199878692627\n",
            "global_step=632400, episodic_return=-0.027577441185712814\n",
            "global_step=633000, episodic_return=-0.032961055636405945\n",
            "global_step=633600, episodic_return=-0.0340481661260128\n",
            "global_step=634200, episodic_return=-0.02336931601166725\n",
            "global_step=634800, episodic_return=-0.02967703528702259\n",
            "global_step=635400, episodic_return=-0.037976861000061035\n",
            "global_step=636000, episodic_return=-0.038500331342220306\n",
            "global_step=636600, episodic_return=-0.031620707362890244\n",
            "global_step=637200, episodic_return=-0.03204399719834328\n",
            "global_step=637800, episodic_return=-0.04741627350449562\n",
            "global_step=638400, episodic_return=-0.04829799011349678\n",
            "global_step=639000, episodic_return=-0.047020040452480316\n",
            "global_step=639600, episodic_return=-0.035559408366680145\n",
            "global_step=640200, episodic_return=-0.03097398206591606\n",
            "global_step=640800, episodic_return=-0.029232025146484375\n",
            "global_step=641400, episodic_return=-0.029358113184571266\n",
            "global_step=642000, episodic_return=-0.011593859642744064\n",
            "global_step=642600, episodic_return=-0.024600893259048462\n",
            "global_step=643200, episodic_return=-0.020442578941583633\n",
            "global_step=643800, episodic_return=-0.011974471621215343\n",
            "global_step=644400, episodic_return=-0.011126106604933739\n",
            "global_step=645000, episodic_return=-0.009047514759004116\n",
            "global_step=645600, episodic_return=-0.027162669226527214\n",
            "global_step=646200, episodic_return=-0.04335551708936691\n",
            "global_step=646800, episodic_return=-0.04542163386940956\n",
            "global_step=647400, episodic_return=-0.03142470493912697\n",
            "global_step=648000, episodic_return=-0.031120404601097107\n",
            "global_step=648600, episodic_return=-0.02792035974562168\n",
            "global_step=649200, episodic_return=-0.024425489827990532\n",
            "global_step=649800, episodic_return=-0.03146062046289444\n",
            "global_step=650400, episodic_return=-0.02339220605790615\n",
            "global_step=651000, episodic_return=-0.008108089677989483\n",
            "global_step=651600, episodic_return=-0.021563056856393814\n",
            "global_step=652200, episodic_return=-0.04005039855837822\n",
            "global_step=652800, episodic_return=-0.03515849635004997\n",
            "global_step=653400, episodic_return=-0.023671401664614677\n",
            "global_step=654000, episodic_return=-0.018772590905427933\n",
            "global_step=654600, episodic_return=-0.013248034752905369\n",
            "global_step=655200, episodic_return=-0.0043088532984256744\n",
            "global_step=655800, episodic_return=-0.01620626635849476\n",
            "global_step=656400, episodic_return=-0.03018355742096901\n",
            "global_step=657000, episodic_return=-0.033612970262765884\n",
            "global_step=657600, episodic_return=-0.014930369332432747\n",
            "global_step=658200, episodic_return=-0.013301966711878777\n",
            "global_step=658800, episodic_return=-0.013964028097689152\n",
            "global_step=659400, episodic_return=-0.012849083170294762\n",
            "global_step=660000, episodic_return=-0.009509398601949215\n"
          ]
        }
      ]
    }
  ]
}